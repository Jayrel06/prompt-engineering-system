{
  "name": "Model Router",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "api/route-prompt",
        "responseMode": "responseNode",
        "options": {
          "allowedOrigins": "*"
        }
      },
      "id": "webhook-trigger",
      "name": "Webhook Trigger",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2,
      "position": [
        240,
        300
      ],
      "webhookId": "route-prompt-webhook"
    },
    {
      "parameters": {
        "content": "## Model Routing Flow\n\nThis workflow:\n1. Receives prompt routing requests\n2. Analyzes task type to determine best model\n3. Routes to appropriate LLM via LiteLLM\n4. Tracks usage and cost\n5. Returns response with metadata",
        "height": 200,
        "width": 320,
        "color": 6
      },
      "id": "sticky-note-1",
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        240,
        80
      ]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "id": "required-prompt",
              "leftValue": "={{ $json.body.prompt }}",
              "rightValue": "",
              "operator": {
                "type": "string",
                "operation": "notEmpty"
              }
            },
            {
              "id": "required-task-type",
              "leftValue": "={{ $json.body.task_type }}",
              "rightValue": "",
              "operator": {
                "type": "string",
                "operation": "notEmpty"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "validate-input",
      "name": "Validate Input",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [
        460,
        300
      ]
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ { \"error\": \"Missing required parameters. Expected: prompt, task_type\" } }}",
        "options": {
          "responseCode": 400
        }
      },
      "id": "error-response",
      "name": "Error Response",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1,
      "position": [
        680,
        460
      ]
    },
    {
      "parameters": {
        "jsCode": "// Determine the best model based on task type\nconst taskType = $input.first().json.body.task_type.toLowerCase();\nconst prompt = $input.first().json.body.prompt;\nconst promptLength = prompt.length;\n\nlet selectedModel = 'gpt-4o-mini'; // default\nlet reasoning = 'Default model for general tasks';\nlet temperature = 0.7;\nlet maxTokens = 2000;\n\n// Model selection logic\nif (taskType === 'code' || taskType === 'coding' || taskType === 'debug') {\n  selectedModel = 'gpt-4o';\n  reasoning = 'Advanced reasoning needed for code tasks';\n  temperature = 0.2;\n  maxTokens = 4000;\n} else if (taskType === 'creative' || taskType === 'writing' || taskType === 'brainstorm') {\n  selectedModel = 'claude-3-5-sonnet-20241022';\n  reasoning = 'Creative tasks benefit from Claude\\'s narrative capabilities';\n  temperature = 0.9;\n  maxTokens = 4000;\n} else if (taskType === 'analysis' || taskType === 'research' || taskType === 'summarize') {\n  selectedModel = 'gpt-4o';\n  reasoning = 'Complex analysis requires advanced reasoning';\n  temperature = 0.3;\n  maxTokens = 3000;\n} else if (taskType === 'quick' || taskType === 'simple' || taskType === 'classify') {\n  selectedModel = 'gpt-4o-mini';\n  reasoning = 'Cost-effective model for simple tasks';\n  temperature = 0.5;\n  maxTokens = 1000;\n} else if (taskType === 'vision' || taskType === 'image') {\n  selectedModel = 'gpt-4o';\n  reasoning = 'Vision capabilities required';\n  temperature = 0.7;\n  maxTokens = 2000;\n} else if (promptLength > 10000) {\n  selectedModel = 'claude-3-5-sonnet-20241022';\n  reasoning = 'Large prompt requires Claude\\'s extended context';\n  temperature = 0.7;\n  maxTokens = 4000;\n}\n\nreturn {\n  prompt: prompt,\n  model: selectedModel,\n  task_type: taskType,\n  reasoning: reasoning,\n  parameters: {\n    temperature: temperature,\n    max_tokens: maxTokens\n  },\n  metadata: {\n    prompt_length: promptLength,\n    timestamp: new Date().toISOString()\n  }\n};"
      },
      "id": "determine-model",
      "name": "Determine Model",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        680,
        180
      ]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://localhost:4000/chat/completions",
        "authentication": "predefinedCredentialType",
        "nodeCredentialType": "liteLlmApi",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "bodyParameters": {
          "parameters": [
            {
              "name": "model",
              "value": "={{ $json.model }}"
            },
            {
              "name": "messages",
              "value": "={{ [{\"role\": \"user\", \"content\": $json.prompt}] }}"
            },
            {
              "name": "temperature",
              "value": "={{ $json.parameters.temperature }}"
            },
            {
              "name": "max_tokens",
              "value": "={{ $json.parameters.max_tokens }}"
            }
          ]
        },
        "options": {}
      },
      "id": "call-litellm",
      "name": "Call LiteLLM",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        900,
        180
      ]
    },
    {
      "parameters": {
        "jsCode": "// Calculate cost estimate based on model and token usage\nconst llmResponse = $input.first().json;\nconst modelInfo = $('Determine Model').first().json;\n\nconst usage = llmResponse.usage || {};\nconst promptTokens = usage.prompt_tokens || 0;\nconst completionTokens = usage.completion_tokens || 0;\nconst totalTokens = usage.total_tokens || 0;\n\n// Cost per 1M tokens (approximate)\nconst costTable = {\n  'gpt-4o': { input: 2.50, output: 10.00 },\n  'gpt-4o-mini': { input: 0.15, output: 0.60 },\n  'claude-3-5-sonnet-20241022': { input: 3.00, output: 15.00 },\n  'gpt-3.5-turbo': { input: 0.50, output: 1.50 }\n};\n\nconst modelCosts = costTable[modelInfo.model] || { input: 1.00, output: 2.00 };\n\nconst inputCost = (promptTokens / 1000000) * modelCosts.input;\nconst outputCost = (completionTokens / 1000000) * modelCosts.output;\nconst totalCost = inputCost + outputCost;\n\nreturn {\n  success: true,\n  response: llmResponse.choices[0].message.content,\n  metadata: {\n    model_used: modelInfo.model,\n    model_reasoning: modelInfo.reasoning,\n    task_type: modelInfo.task_type,\n    usage: {\n      prompt_tokens: promptTokens,\n      completion_tokens: completionTokens,\n      total_tokens: totalTokens\n    },\n    cost_estimate: {\n      input_cost_usd: inputCost.toFixed(6),\n      output_cost_usd: outputCost.toFixed(6),\n      total_cost_usd: totalCost.toFixed(6)\n    },\n    parameters: modelInfo.parameters,\n    timestamp: new Date().toISOString(),\n    request_id: llmResponse.id || `req-${Date.now()}`\n  }\n};"
      },
      "id": "calculate-cost",
      "name": "Calculate Cost",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1120,
        180
      ]
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ $json }}",
        "options": {}
      },
      "id": "success-response",
      "name": "Success Response",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1,
      "position": [
        1340,
        180
      ]
    },
    {
      "parameters": {
        "jsCode": "// Handle LLM API errors\nconst error = $input.first().json;\nconst modelInfo = $('Determine Model').first().json;\n\nreturn {\n  success: false,\n  error: \"Failed to get LLM response\",\n  details: error.error || error.message || \"Unknown error\",\n  model_attempted: modelInfo.model,\n  task_type: modelInfo.task_type,\n  timestamp: new Date().toISOString()\n};"
      },
      "id": "handle-llm-error",
      "name": "Handle LLM Error",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1120,
        360
      ]
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ $json }}",
        "options": {
          "responseCode": 500
        }
      },
      "id": "llm-error-response",
      "name": "LLM Error Response",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1,
      "position": [
        1340,
        360
      ]
    },
    {
      "parameters": {
        "content": "## Model Selection Strategy\n\n- **Code/Debug**: GPT-4o (reasoning)\n- **Creative/Writing**: Claude Sonnet (narrative)\n- **Analysis/Research**: GPT-4o (analysis)\n- **Quick/Simple**: GPT-4o-mini (cost)\n- **Long prompts**: Claude (context)\n- **Vision**: GPT-4o (multimodal)",
        "height": 260,
        "width": 280,
        "color": 3
      },
      "id": "sticky-note-2",
      "name": "Sticky Note 2",
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        680,
        -120
      ]
    }
  ],
  "connections": {
    "Webhook Trigger": {
      "main": [
        [
          {
            "node": "Validate Input",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Validate Input": {
      "main": [
        [
          {
            "node": "Determine Model",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Error Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Determine Model": {
      "main": [
        [
          {
            "node": "Call LiteLLM",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call LiteLLM": {
      "main": [
        [
          {
            "node": "Calculate Cost",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Calculate Cost": {
      "main": [
        [
          {
            "node": "Success Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Success Response": {
      "main": []
    },
    "Error Response": {
      "main": []
    },
    "Handle LLM Error": {
      "main": [
        [
          {
            "node": "LLM Error Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "LLM Error Response": {
      "main": []
    }
  },
  "pinData": {},
  "settings": {
    "executionOrder": "v1"
  },
  "staticData": null,
  "tags": [],
  "triggerCount": 1,
  "updatedAt": "2025-11-27T00:00:00.000Z",
  "versionId": "1"
}
